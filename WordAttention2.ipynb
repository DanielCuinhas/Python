{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WordAttention2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOAgiEyVzYu/BvZTIslZLxC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"otqcwDSo6JR6","colab_type":"code","outputId":"1f123198-a133-4708-c1f5-eaf2fcb8449e","executionInfo":{"status":"ok","timestamp":1584313264371,"user_tz":-60,"elapsed":1064,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eqQmGpGA6xFH","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# read data from text files\n","with open('/content/drive/My Drive/Colab Notebooks/Sentiment/data/reviews.txt', 'r') as f:\n","    reviews = f.read()\n","with open('/content/drive/My Drive/Colab Notebooks/Sentiment/data/labels.txt', 'r') as f:\n","    labels = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOLDNKE_6QVp","colab_type":"code","colab":{}},"source":["from string import punctuation\n","\n","# get rid of punctuation\n","reviews = reviews.lower() # lowercase, standardize\n","all_text = ''.join([c for c in reviews if c not in punctuation])\n","\n","# split by new lines and spaces\n","reviews_split = all_text.split('\\n')\n","all_text = ' '.join(reviews_split)\n","\n","# create a list of words\n","words = all_text.split()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0O6CUXM6uXy","colab_type":"code","colab":{}},"source":["# feel free to use this import \n","from collections import Counter\n","\n","## Build a dictionary that maps words to integers\n","counts = Counter(words)\n","vocab = sorted(counts, key=counts.get, reverse=True)\n","vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n","vocab_to_int['<pad>']=0\n","\n","int2vocab={i:w for w,i in vocab_to_int.items()}\n","\n","## use the dict to tokenize each review in reviews_split\n","## store the tokenized reviews in reviews_ints\n","reviews_ints = []\n","for review in reviews_split:\n","    reviews_ints.append([vocab_to_int[word] for word in review.split()])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEAEN6SM616e","colab_type":"code","colab":{}},"source":["# 1=positive, 0=negative label conversion\n","labels_split = labels.split('\\n')\n","encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdrBjtJrK1El","colab_type":"code","colab":{}},"source":["non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n","\n","# remove 0-length reviews and their labels\n","reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n","encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cc1aFvln64Mc","colab_type":"code","colab":{}},"source":["review_lengths = np.array(list(map(len,reviews_ints)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3ZYd2_C8y5D","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWbPpr5L84vl","colab_type":"code","colab":{}},"source":["def pad_features(reviews_ints, seq_length):\n","    ''' Return features of review_ints, where each review is padded with 0's \n","        or truncated to the input seq_length.\n","    '''\n","    \n","    # getting the correct rows x cols shape\n","    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n","\n","    # for each review, I grab that review and \n","    for i, row in enumerate(reviews_ints):\n","        features[i, :len(row)] = np.array(row)[:seq_length]\n","    \n","    return features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1seckIQI84yV","colab_type":"code","colab":{}},"source":["\n","seq_length = max(review_lengths)\n","# seq_length = int(np.percentile(review_lengths,95))\n","features = pad_features(reviews_ints,seq_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEX6thxzOArM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNAInDEFOAoo","colab_type":"code","colab":{}},"source":["review_lengths = np.sum(features!=0,axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcmZsHHzOJXY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ml4DG7pj70gr","colab_type":"code","colab":{}},"source":["from torch.utils.data import TensorDataset, DataLoader\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubGsP3ze846n","colab_type":"code","outputId":"624f96a9-3b79-49e2-f71b-45c2ad654e4d","executionInfo":{"status":"ok","timestamp":1584313270692,"user_tz":-60,"elapsed":7191,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["split_frac = 0.8\n","\n","## split data into training, validation, and test data (features and labels, x and y)\n","\n","split_idx = int(len(features)*split_frac)\n","train_x, remaining_x = features[:split_idx], features[split_idx:]\n","train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","train_length, remaining_length=review_lengths[:split_idx], review_lengths[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","val_legth, test_length = remaining_length[:test_idx], remaining_length[test_idx:]\n","\n","## print out the shapes of your resultant feature data\n","print(\"\\t\\t\\tFeature Shapes:\")\n","print(\"Train set: \\t\\t{}\".format(train_x.shape), \n","      \"\\nValidation set: \\t{}\".format(val_x.shape),\n","      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["\t\t\tFeature Shapes:\n","Train set: \t\t(20000, 2514) \n","Validation set: \t(2500, 2514) \n","Test set: \t\t(2500, 2514)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19Oc6GBp7CXb","colab_type":"code","colab":{}},"source":["train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_length), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_legth), torch.from_numpy(val_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_length), torch.from_numpy(test_y))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBYZFXlD55aB","colab_type":"code","colab":{}},"source":["def my_collate(batch):\n","    # batch contains a list of tuples of structure (sequence, target)\n","    data = [item[0] for item in batch]\n","    lengths = [item[1] for item in batch]\n","    targets = [item[2] for item in batch]\n","    tuples=list(zip(data, lengths, targets))\n","    tuples = sorted(tuples, key=lambda s: s[1],reverse=True)\n","    data = [item[0] for item in tuples]\n","    lengths = [item[1] for item in tuples]\n","    targets = [item[2] for item in tuples]\n","    return [torch.stack(data), torch.stack(lengths), torch.stack(targets)]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"raetxkx0A-y1","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGhwKqoz_jHt","colab_type":"code","colab":{}},"source":["# from torchtext import data\n","# from torchtext import datasets\n","# # dataloaders\n","\n","\n","# train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","#     (train_data, valid_data, test_data),\n","#     sort_key = lambda x: x.s, #sort by s attribute (quote)\n","#     batch_size=BATCH_SIZE,\n","#     device=device)\n","\n","\n","BATCH_SIZE = 64\n","# make sure the SHUFFLE your training data\n","train_loader = DataLoader(train_data, shuffle=True,batch_size=BATCH_SIZE)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_data, shuffle=True,batch_size=BATCH_SIZE)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"98Z8Z0HkBE51","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n","\n","class WordAttention(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, att_size, output_dim, dropout):\n","        super(WordAttention, self).__init__()\n","\n","        self.emb = nn.Embedding(vocab_size, embedding_dim)\n","        \n","        self.rnn = nn.GRU(embedding_dim,\n","                          hidden_dim, \n","                          num_layers=n_layers, \n","                          bidirectional=True,\n","                          dropout = 0 if n_layers < 2 else dropout,\n","                          batch_first=True)\n","        \n","        self.att = nn.Linear(2 * hidden_dim, att_size)\n","        \n","        self.context_vector = nn.Linear(att_size, 1, bias=False)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def forward(self, text, text_lengths):\n","        #text = [batch size, sent len]\n","\n","        text=text.long()\n","        text_lengths = text_lengths.long()\n","\n","        embedded = self.dropout(self.emb(text))  \n","        \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        packed_words = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n","        \n","        packed_words, _ = self.rnn(packed_words)\n","        \n","        #packed_words.data = [n words, hid dim * num directions]\n","        \n","        att_w = torch.tanh(self.att(packed_words.data))\n","        \n","        #att_w = [n words, att size]\n","            \n","        att_w = self.context_vector(att_w).squeeze(1)\n","        \n","        #att_w = [n words]\n","        \n","        max_value = att_w.max()  # scalar, for numerical stability during exponent calculation\n","        \n","        att_w = torch.exp(att_w - max_value)\n","        \n","        #att_w = [n words]\n","        \n","        att_w, _ = pad_packed_sequence(PackedSequence(data=att_w,\n","                                                      batch_sizes=packed_words.batch_sizes,\n","                                                      sorted_indices=packed_words.sorted_indices,\n","                                                      unsorted_indices=packed_words.unsorted_indices),\n","                                       batch_first=True)  \n","        \n","        #att_w = [batch size, max(text_lengths)]\n","        \n","        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True)\n","        \n","        #word_alphas = [batch size, max(text_lengths)]\n","        \n","        sentences, _ = pad_packed_sequence(packed_words,\n","                                           batch_first=True)\n","        \n","        #sentences = [batch size, max(text_lengths), hid dim * num directions ]\n","        \n","        sentences = sentences * word_alphas.unsqueeze(2)\n","        \n","        #sentences = [batch size, max(text_lengths), hid dim * num directions ]\n","        \n","        sentences = sentences.sum(dim=1)\n","        \n","        #sentences = [batch size, hid dim * num directions]\n","        \n","        output = self.fc(sentences)\n","        \n","        #output = [batch size, output dim]\n","        \n","        return output\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oYVWH0XBGkC","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(vocab_to_int)\n","EMBEDDING_DIM = 100\n","OUTPUT_DIM = 1\n","HIDDEN_DIM = 100\n","N_LAYERS = 1\n","ATT_DIM = 100\n","DROPOUT = 0.5\n","# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model=WordAttention(INPUT_DIM, \n","                    EMBEDDING_DIM, \n","                    HIDDEN_DIM, \n","                    N_LAYERS, \n","                    ATT_DIM,\n","                    OUTPUT_DIM,\n","                    DROPOUT)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I42_JQotBdIf","colab_type":"code","outputId":"c659d03d-436e-4591-8f7c-e4ef26b7b2c2","executionInfo":{"status":"ok","timestamp":1584313270695,"user_tz":-60,"elapsed":7119,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["The model has 7,548,901 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VBiXNow2Bfnn","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFXtoYDnBhWf","colab_type":"code","colab":{}},"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGB7ke1kBjrf","colab_type":"code","colab":{}},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"my6h7j7WBlcM","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for text, text_lengths, label in iterator:\n","        text, text_lengths, label=text.cuda(), text_lengths.cuda(), label.cuda()\n","\n","        optimizer.zero_grad()\n","      \n","        predictions = model(text, text_lengths).squeeze(1)\n","        \n","        loss = criterion(predictions, label.float())\n","        \n","        acc = binary_accuracy(predictions, label.float())\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lcZGBiNBqJO","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for text, text_lengths, label in iterator:\n","            text, text_lengths, label=text.cuda(), text_lengths.cuda(), label.cuda()\n","            \n","            predictions = model(text, text_lengths).squeeze(1)\n","            \n","            loss = criterion(predictions, label.float())\n","            \n","            acc = binary_accuracy(predictions, label.float())\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cv3c5gpyBq96","colab_type":"code","colab":{}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxt5pJZPBzPI","colab_type":"code","outputId":"cf5bb55a-52a1-418d-b8a7-da0c017037cf","executionInfo":{"status":"ok","timestamp":1584313620130,"user_tz":-60,"elapsed":252250,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["N_EPOCHS = 20\n","\n","best_valid_loss = float('inf')\n","counter = 0\n","patience = 2\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","    \n","    if valid_loss < best_valid_loss:\n","      best_valid_loss = valid_loss\n","      torch.save(model.state_dict(), 'tut2-model.pt')\n","      counter = 0 \n","    else:\n","      counter += 1\n","      if counter >= patience:\n","          break\n","    \n","    "],"execution_count":26,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 48s\n","\tTrain Loss: 0.569 | Train Acc: 69.61%\n","\t Val. Loss: 0.527 |  Val. Acc: 76.05%\n","Epoch: 02 | Epoch Time: 0m 49s\n","\tTrain Loss: 0.418 | Train Acc: 81.03%\n","\t Val. Loss: 0.460 |  Val. Acc: 82.93%\n","Epoch: 03 | Epoch Time: 0m 49s\n","\tTrain Loss: 0.340 | Train Acc: 85.44%\n","\t Val. Loss: 0.393 |  Val. Acc: 85.04%\n","Epoch: 04 | Epoch Time: 0m 49s\n","\tTrain Loss: 0.281 | Train Acc: 88.60%\n","\t Val. Loss: 0.400 |  Val. Acc: 87.19%\n","Epoch: 05 | Epoch Time: 0m 49s\n","\tTrain Loss: 0.250 | Train Acc: 89.93%\n","\t Val. Loss: 0.371 |  Val. Acc: 87.73%\n","Epoch: 06 | Epoch Time: 0m 49s\n","\tTrain Loss: 0.222 | Train Acc: 91.34%\n","\t Val. Loss: 0.417 |  Val. Acc: 86.95%\n","Epoch: 07 | Epoch Time: 0m 49s\n","\tTrain Loss: 0.198 | Train Acc: 92.32%\n","\t Val. Loss: 0.409 |  Val. Acc: 87.54%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8a164kIdCViH","colab_type":"code","outputId":"67b17949-955a-4777-ff3e-1427fe16766c","executionInfo":{"status":"ok","timestamp":1584313620614,"user_tz":-60,"elapsed":489,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# model.load_state_dict(torch.load('tut2-model.pt'))\n","test_loss, test_acc = evaluate(model, test_loader, criterion)\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Test Loss: 0.405 | Test Acc: 86.88%\n"],"name":"stdout"}]}]}