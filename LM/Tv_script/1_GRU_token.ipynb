{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq8D3alTccp-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPheuBvUda-x"
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(path='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9enJzeBddQ1"
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]#.view(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXQsgOLVdiED"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_layers, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "\n",
    "        #inputs = [batch size]\n",
    "\n",
    "        encoded = self.drop(self.embedding(inputs))\n",
    "\n",
    "        #encoded = [batch size, emb dim]\n",
    "\n",
    "        encoded = encoded.unsqueeze(0)\n",
    "\n",
    "        #encoded = [1, batch size, emb dim]\n",
    "\n",
    "        output, hidden = self.gru(encoded, hidden)\n",
    "\n",
    "        #output = [1, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "\n",
    "        output = self.drop(output)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        #output = [1, batch size, input size]\n",
    "\n",
    "        output = output.view(-1, self.input_size)\n",
    "\n",
    "        #output = [1*batch size, input size]\n",
    "\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eERJsurieCBG"
   },
   "outputs": [],
   "source": [
    "n_characters  = len(corpus.dictionary)\n",
    "emb_size = 200\n",
    "hidden_size = 250\n",
    "n_layers = 2\n",
    "lr = 0.001\n",
    "dropout = 0.5\n",
    "\n",
    "model = RNN(n_characters, emb_size, hidden_size, n_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2904,
     "status": "ok",
     "timestamp": 1591547990594,
     "user": {
      "displayName": "Daniel Cui침as V치zquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "nNdALNEVeHh_",
    "outputId": "01ad4247-32c2-4771-8c75-dfa627885a18"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 21,627,468 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2893,
     "status": "ok",
     "timestamp": 1591547990595,
     "user": {
      "displayName": "Daniel Cui침as V치zquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "qFF-wwjueLu9",
    "outputId": "464cc750-849b-44e0-c7ab-f7a55abf4a01"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vyo6VRkAeTdx"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvDeI7LCeVgX"
   },
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "criterion=criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oak9TZkoeXIT"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "bptt = 10\n",
    "\n",
    "train_loader = batchify(corpus.train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BejRnkqTedIT"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion):\n",
    "    clip = 0.25\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    # all_loss = []\n",
    "\n",
    "    for batch, i in enumerate(range(0, iterator.size(0) - 1, bptt)):\n",
    "        loss = 0\n",
    "        data, targets = get_batch(iterator, i)\n",
    "\n",
    "        seq_len_batched = data.shape[0]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "\n",
    "        for c in range(seq_len_batched):\n",
    "            output, hidden = model(data[c], hidden)\n",
    "            loss += criterion(output, targets[c])\n",
    "      \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss = loss.item()/seq_len_batched\n",
    "        \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss #/ log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(iterator) // bptt, lr,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            # total_loss = 0\n",
    "            # print(generate(model, 'Wh', 100), '\\n')\n",
    "\n",
    "        # if batch % plot_every == 0:\n",
    "        #     all_loss.append(total_loss)\n",
    "        # return all_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6XEnI13efnC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4120927,
     "status": "ok",
     "timestamp": 1591552108678,
     "user": {
      "displayName": "Daniel Cui침as V치zquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "lynLqiinehJr",
    "outputId": "3e8d5c69-2873-4da4-f4e9-6db9ce908805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   100/  558 batches | lr 0.00 | loss  6.73 | ppl   836.72\n",
      "| epoch   0 |   200/  558 batches | lr 0.00 | loss  6.69 | ppl   803.45\n",
      "| epoch   0 |   300/  558 batches | lr 0.00 | loss  6.60 | ppl   735.15\n",
      "| epoch   0 |   400/  558 batches | lr 0.00 | loss  6.61 | ppl   744.06\n",
      "| epoch   0 |   500/  558 batches | lr 0.00 | loss  6.29 | ppl   541.52\n",
      "Epoch: 01 | Epoch Time: 1m 22s\n",
      "| epoch   1 |   100/  558 batches | lr 0.00 | loss  6.12 | ppl   453.20\n",
      "| epoch   1 |   200/  558 batches | lr 0.00 | loss  6.17 | ppl   476.75\n",
      "| epoch   1 |   300/  558 batches | lr 0.00 | loss  6.14 | ppl   465.73\n",
      "| epoch   1 |   400/  558 batches | lr 0.00 | loss  6.14 | ppl   462.57\n",
      "| epoch   1 |   500/  558 batches | lr 0.00 | loss  5.85 | ppl   346.31\n",
      "Epoch: 02 | Epoch Time: 1m 22s\n",
      "| epoch   2 |   100/  558 batches | lr 0.00 | loss  5.61 | ppl   272.84\n",
      "| epoch   2 |   200/  558 batches | lr 0.00 | loss  5.77 | ppl   319.38\n",
      "| epoch   2 |   300/  558 batches | lr 0.00 | loss  5.67 | ppl   290.70\n",
      "| epoch   2 |   400/  558 batches | lr 0.00 | loss  5.73 | ppl   308.99\n",
      "| epoch   2 |   500/  558 batches | lr 0.00 | loss  5.47 | ppl   238.51\n",
      "Epoch: 03 | Epoch Time: 1m 22s\n",
      "| epoch   3 |   100/  558 batches | lr 0.00 | loss  5.39 | ppl   218.66\n",
      "| epoch   3 |   200/  558 batches | lr 0.00 | loss  5.49 | ppl   243.02\n",
      "| epoch   3 |   300/  558 batches | lr 0.00 | loss  5.46 | ppl   235.39\n",
      "| epoch   3 |   400/  558 batches | lr 0.00 | loss  5.52 | ppl   250.05\n",
      "| epoch   3 |   500/  558 batches | lr 0.00 | loss  5.26 | ppl   193.05\n",
      "Epoch: 04 | Epoch Time: 1m 22s\n",
      "| epoch   4 |   100/  558 batches | lr 0.00 | loss  5.22 | ppl   185.56\n",
      "| epoch   4 |   200/  558 batches | lr 0.00 | loss  5.37 | ppl   214.18\n",
      "| epoch   4 |   300/  558 batches | lr 0.00 | loss  5.33 | ppl   207.08\n",
      "| epoch   4 |   400/  558 batches | lr 0.00 | loss  5.44 | ppl   229.38\n",
      "| epoch   4 |   500/  558 batches | lr 0.00 | loss  5.13 | ppl   169.49\n",
      "Epoch: 05 | Epoch Time: 1m 22s\n",
      "| epoch   5 |   100/  558 batches | lr 0.00 | loss  5.09 | ppl   162.54\n",
      "| epoch   5 |   200/  558 batches | lr 0.00 | loss  5.25 | ppl   191.34\n",
      "| epoch   5 |   300/  558 batches | lr 0.00 | loss  5.22 | ppl   184.67\n",
      "| epoch   5 |   400/  558 batches | lr 0.00 | loss  5.29 | ppl   199.12\n",
      "| epoch   5 |   500/  558 batches | lr 0.00 | loss  4.99 | ppl   146.68\n",
      "Epoch: 06 | Epoch Time: 1m 22s\n",
      "| epoch   6 |   100/  558 batches | lr 0.00 | loss  4.98 | ppl   145.71\n",
      "| epoch   6 |   200/  558 batches | lr 0.00 | loss  5.13 | ppl   169.72\n",
      "| epoch   6 |   300/  558 batches | lr 0.00 | loss  5.13 | ppl   168.78\n",
      "| epoch   6 |   400/  558 batches | lr 0.00 | loss  5.18 | ppl   177.42\n",
      "| epoch   6 |   500/  558 batches | lr 0.00 | loss  4.90 | ppl   133.78\n",
      "Epoch: 07 | Epoch Time: 1m 21s\n",
      "| epoch   7 |   100/  558 batches | lr 0.00 | loss  4.89 | ppl   133.32\n",
      "| epoch   7 |   200/  558 batches | lr 0.00 | loss  5.04 | ppl   154.99\n",
      "| epoch   7 |   300/  558 batches | lr 0.00 | loss  5.02 | ppl   151.14\n",
      "| epoch   7 |   400/  558 batches | lr 0.00 | loss  5.11 | ppl   165.00\n",
      "| epoch   7 |   500/  558 batches | lr 0.00 | loss  4.83 | ppl   124.65\n",
      "Epoch: 08 | Epoch Time: 1m 22s\n",
      "| epoch   8 |   100/  558 batches | lr 0.00 | loss  4.81 | ppl   122.91\n",
      "| epoch   8 |   200/  558 batches | lr 0.00 | loss  4.95 | ppl   141.43\n",
      "| epoch   8 |   300/  558 batches | lr 0.00 | loss  4.95 | ppl   141.60\n",
      "| epoch   8 |   400/  558 batches | lr 0.00 | loss  4.99 | ppl   146.85\n",
      "| epoch   8 |   500/  558 batches | lr 0.00 | loss  4.72 | ppl   112.44\n",
      "Epoch: 09 | Epoch Time: 1m 22s\n",
      "| epoch   9 |   100/  558 batches | lr 0.00 | loss  4.71 | ppl   111.41\n",
      "| epoch   9 |   200/  558 batches | lr 0.00 | loss  4.85 | ppl   127.75\n",
      "| epoch   9 |   300/  558 batches | lr 0.00 | loss  4.83 | ppl   125.65\n",
      "| epoch   9 |   400/  558 batches | lr 0.00 | loss  4.92 | ppl   137.45\n",
      "| epoch   9 |   500/  558 batches | lr 0.00 | loss  4.65 | ppl   104.28\n",
      "Epoch: 10 | Epoch Time: 1m 22s\n",
      "| epoch  10 |   100/  558 batches | lr 0.00 | loss  4.63 | ppl   102.03\n",
      "| epoch  10 |   200/  558 batches | lr 0.00 | loss  4.84 | ppl   126.00\n",
      "| epoch  10 |   300/  558 batches | lr 0.00 | loss  4.79 | ppl   120.70\n",
      "| epoch  10 |   400/  558 batches | lr 0.00 | loss  4.80 | ppl   122.07\n",
      "| epoch  10 |   500/  558 batches | lr 0.00 | loss  4.56 | ppl    95.86\n",
      "Epoch: 11 | Epoch Time: 1m 22s\n",
      "| epoch  11 |   100/  558 batches | lr 0.00 | loss  4.64 | ppl   104.05\n",
      "| epoch  11 |   200/  558 batches | lr 0.00 | loss  4.74 | ppl   114.22\n",
      "| epoch  11 |   300/  558 batches | lr 0.00 | loss  4.66 | ppl   105.19\n",
      "| epoch  11 |   400/  558 batches | lr 0.00 | loss  4.74 | ppl   114.94\n",
      "| epoch  11 |   500/  558 batches | lr 0.00 | loss  4.52 | ppl    91.76\n",
      "Epoch: 12 | Epoch Time: 1m 22s\n",
      "| epoch  12 |   100/  558 batches | lr 0.00 | loss  4.50 | ppl    90.06\n",
      "| epoch  12 |   200/  558 batches | lr 0.00 | loss  4.68 | ppl   107.47\n",
      "| epoch  12 |   300/  558 batches | lr 0.00 | loss  4.64 | ppl   103.26\n",
      "| epoch  12 |   400/  558 batches | lr 0.00 | loss  4.70 | ppl   110.47\n",
      "| epoch  12 |   500/  558 batches | lr 0.00 | loss  4.46 | ppl    86.35\n",
      "Epoch: 13 | Epoch Time: 1m 22s\n",
      "| epoch  13 |   100/  558 batches | lr 0.00 | loss  4.43 | ppl    84.28\n",
      "| epoch  13 |   200/  558 batches | lr 0.00 | loss  4.58 | ppl    97.59\n",
      "| epoch  13 |   300/  558 batches | lr 0.00 | loss  4.58 | ppl    97.15\n",
      "| epoch  13 |   400/  558 batches | lr 0.00 | loss  4.60 | ppl    99.24\n",
      "| epoch  13 |   500/  558 batches | lr 0.00 | loss  4.31 | ppl    74.67\n",
      "Epoch: 14 | Epoch Time: 1m 22s\n",
      "| epoch  14 |   100/  558 batches | lr 0.00 | loss  4.36 | ppl    77.91\n",
      "| epoch  14 |   200/  558 batches | lr 0.00 | loss  4.55 | ppl    94.41\n",
      "| epoch  14 |   300/  558 batches | lr 0.00 | loss  4.49 | ppl    89.45\n",
      "| epoch  14 |   400/  558 batches | lr 0.00 | loss  4.55 | ppl    94.84\n",
      "| epoch  14 |   500/  558 batches | lr 0.00 | loss  4.36 | ppl    78.03\n",
      "Epoch: 15 | Epoch Time: 1m 22s\n",
      "| epoch  15 |   100/  558 batches | lr 0.00 | loss  4.37 | ppl    78.70\n",
      "| epoch  15 |   200/  558 batches | lr 0.00 | loss  4.48 | ppl    88.27\n",
      "| epoch  15 |   300/  558 batches | lr 0.00 | loss  4.44 | ppl    85.17\n",
      "| epoch  15 |   400/  558 batches | lr 0.00 | loss  4.50 | ppl    90.08\n",
      "| epoch  15 |   500/  558 batches | lr 0.00 | loss  4.27 | ppl    71.69\n",
      "Epoch: 16 | Epoch Time: 1m 22s\n",
      "| epoch  16 |   100/  558 batches | lr 0.00 | loss  4.26 | ppl    71.06\n",
      "| epoch  16 |   200/  558 batches | lr 0.00 | loss  4.39 | ppl    80.73\n",
      "| epoch  16 |   300/  558 batches | lr 0.00 | loss  4.38 | ppl    79.73\n",
      "| epoch  16 |   400/  558 batches | lr 0.00 | loss  4.48 | ppl    88.43\n",
      "| epoch  16 |   500/  558 batches | lr 0.00 | loss  4.19 | ppl    66.26\n",
      "Epoch: 17 | Epoch Time: 1m 22s\n",
      "| epoch  17 |   100/  558 batches | lr 0.00 | loss  4.23 | ppl    68.78\n",
      "| epoch  17 |   200/  558 batches | lr 0.00 | loss  4.32 | ppl    74.89\n",
      "| epoch  17 |   300/  558 batches | lr 0.00 | loss  4.36 | ppl    78.22\n",
      "| epoch  17 |   400/  558 batches | lr 0.00 | loss  4.35 | ppl    77.61\n",
      "| epoch  17 |   500/  558 batches | lr 0.00 | loss  4.13 | ppl    62.40\n",
      "Epoch: 18 | Epoch Time: 1m 22s\n",
      "| epoch  18 |   100/  558 batches | lr 0.00 | loss  4.15 | ppl    63.39\n",
      "| epoch  18 |   200/  558 batches | lr 0.00 | loss  4.34 | ppl    76.73\n",
      "| epoch  18 |   300/  558 batches | lr 0.00 | loss  4.28 | ppl    72.35\n",
      "| epoch  18 |   400/  558 batches | lr 0.00 | loss  4.37 | ppl    79.22\n",
      "| epoch  18 |   500/  558 batches | lr 0.00 | loss  4.03 | ppl    56.29\n",
      "Epoch: 19 | Epoch Time: 1m 22s\n",
      "| epoch  19 |   100/  558 batches | lr 0.00 | loss  4.14 | ppl    62.84\n",
      "| epoch  19 |   200/  558 batches | lr 0.00 | loss  4.30 | ppl    73.99\n",
      "| epoch  19 |   300/  558 batches | lr 0.00 | loss  4.25 | ppl    70.41\n",
      "| epoch  19 |   400/  558 batches | lr 0.00 | loss  4.30 | ppl    73.52\n",
      "| epoch  19 |   500/  558 batches | lr 0.00 | loss  4.03 | ppl    56.41\n",
      "Epoch: 20 | Epoch Time: 1m 22s\n",
      "| epoch  20 |   100/  558 batches | lr 0.00 | loss  4.04 | ppl    57.07\n",
      "| epoch  20 |   200/  558 batches | lr 0.00 | loss  4.22 | ppl    68.08\n",
      "| epoch  20 |   300/  558 batches | lr 0.00 | loss  4.18 | ppl    65.31\n",
      "| epoch  20 |   400/  558 batches | lr 0.00 | loss  4.27 | ppl    71.87\n",
      "| epoch  20 |   500/  558 batches | lr 0.00 | loss  4.01 | ppl    55.21\n",
      "Epoch: 21 | Epoch Time: 1m 22s\n",
      "| epoch  21 |   100/  558 batches | lr 0.00 | loss  4.01 | ppl    55.11\n",
      "| epoch  21 |   200/  558 batches | lr 0.00 | loss  4.22 | ppl    67.77\n",
      "| epoch  21 |   300/  558 batches | lr 0.00 | loss  4.13 | ppl    62.30\n",
      "| epoch  21 |   400/  558 batches | lr 0.00 | loss  4.17 | ppl    64.67\n",
      "| epoch  21 |   500/  558 batches | lr 0.00 | loss  3.97 | ppl    53.04\n",
      "Epoch: 22 | Epoch Time: 1m 22s\n",
      "| epoch  22 |   100/  558 batches | lr 0.00 | loss  4.01 | ppl    55.42\n",
      "| epoch  22 |   200/  558 batches | lr 0.00 | loss  4.15 | ppl    63.15\n",
      "| epoch  22 |   300/  558 batches | lr 0.00 | loss  4.17 | ppl    64.51\n",
      "| epoch  22 |   400/  558 batches | lr 0.00 | loss  4.22 | ppl    67.98\n",
      "| epoch  22 |   500/  558 batches | lr 0.00 | loss  3.91 | ppl    49.83\n",
      "Epoch: 23 | Epoch Time: 1m 22s\n",
      "| epoch  23 |   100/  558 batches | lr 0.00 | loss  3.96 | ppl    52.59\n",
      "| epoch  23 |   200/  558 batches | lr 0.00 | loss  4.13 | ppl    62.10\n",
      "| epoch  23 |   300/  558 batches | lr 0.00 | loss  4.09 | ppl    59.99\n",
      "| epoch  23 |   400/  558 batches | lr 0.00 | loss  4.19 | ppl    66.16\n",
      "| epoch  23 |   500/  558 batches | lr 0.00 | loss  3.87 | ppl    48.05\n",
      "Epoch: 24 | Epoch Time: 1m 22s\n",
      "| epoch  24 |   100/  558 batches | lr 0.00 | loss  3.98 | ppl    53.64\n",
      "| epoch  24 |   200/  558 batches | lr 0.00 | loss  4.10 | ppl    60.09\n",
      "| epoch  24 |   300/  558 batches | lr 0.00 | loss  4.05 | ppl    57.23\n",
      "| epoch  24 |   400/  558 batches | lr 0.00 | loss  4.12 | ppl    61.55\n",
      "| epoch  24 |   500/  558 batches | lr 0.00 | loss  3.86 | ppl    47.33\n",
      "Epoch: 25 | Epoch Time: 1m 22s\n",
      "| epoch  25 |   100/  558 batches | lr 0.00 | loss  3.92 | ppl    50.47\n",
      "| epoch  25 |   200/  558 batches | lr 0.00 | loss  4.06 | ppl    58.20\n",
      "| epoch  25 |   300/  558 batches | lr 0.00 | loss  4.03 | ppl    56.24\n",
      "| epoch  25 |   400/  558 batches | lr 0.00 | loss  4.03 | ppl    56.37\n",
      "| epoch  25 |   500/  558 batches | lr 0.00 | loss  3.81 | ppl    45.29\n",
      "Epoch: 26 | Epoch Time: 1m 22s\n",
      "| epoch  26 |   100/  558 batches | lr 0.00 | loss  3.90 | ppl    49.38\n",
      "| epoch  26 |   200/  558 batches | lr 0.00 | loss  4.05 | ppl    57.23\n",
      "| epoch  26 |   300/  558 batches | lr 0.00 | loss  4.02 | ppl    55.62\n",
      "| epoch  26 |   400/  558 batches | lr 0.00 | loss  4.00 | ppl    54.83\n",
      "| epoch  26 |   500/  558 batches | lr 0.00 | loss  3.79 | ppl    44.42\n",
      "Epoch: 27 | Epoch Time: 1m 22s\n",
      "| epoch  27 |   100/  558 batches | lr 0.00 | loss  3.93 | ppl    50.68\n",
      "| epoch  27 |   200/  558 batches | lr 0.00 | loss  4.00 | ppl    54.63\n",
      "| epoch  27 |   300/  558 batches | lr 0.00 | loss  3.95 | ppl    52.14\n",
      "| epoch  27 |   400/  558 batches | lr 0.00 | loss  4.06 | ppl    58.06\n",
      "| epoch  27 |   500/  558 batches | lr 0.00 | loss  3.75 | ppl    42.64\n",
      "Epoch: 28 | Epoch Time: 1m 22s\n",
      "| epoch  28 |   100/  558 batches | lr 0.00 | loss  3.82 | ppl    45.68\n",
      "| epoch  28 |   200/  558 batches | lr 0.00 | loss  4.02 | ppl    55.88\n",
      "| epoch  28 |   300/  558 batches | lr 0.00 | loss  3.91 | ppl    49.85\n",
      "| epoch  28 |   400/  558 batches | lr 0.00 | loss  4.02 | ppl    55.92\n",
      "| epoch  28 |   500/  558 batches | lr 0.00 | loss  3.76 | ppl    42.83\n",
      "Epoch: 29 | Epoch Time: 1m 22s\n",
      "| epoch  29 |   100/  558 batches | lr 0.00 | loss  3.85 | ppl    46.98\n",
      "| epoch  29 |   200/  558 batches | lr 0.00 | loss  3.94 | ppl    51.31\n",
      "| epoch  29 |   300/  558 batches | lr 0.00 | loss  3.93 | ppl    51.06\n",
      "| epoch  29 |   400/  558 batches | lr 0.00 | loss  3.97 | ppl    53.16\n",
      "| epoch  29 |   500/  558 batches | lr 0.00 | loss  3.73 | ppl    41.68\n",
      "Epoch: 30 | Epoch Time: 1m 22s\n",
      "| epoch  30 |   100/  558 batches | lr 0.00 | loss  3.83 | ppl    46.04\n",
      "| epoch  30 |   200/  558 batches | lr 0.00 | loss  3.90 | ppl    49.19\n",
      "| epoch  30 |   300/  558 batches | lr 0.00 | loss  3.95 | ppl    52.00\n",
      "| epoch  30 |   400/  558 batches | lr 0.00 | loss  3.97 | ppl    52.87\n",
      "| epoch  30 |   500/  558 batches | lr 0.00 | loss  3.77 | ppl    43.42\n",
      "Epoch: 31 | Epoch Time: 1m 22s\n",
      "| epoch  31 |   100/  558 batches | lr 0.00 | loss  3.82 | ppl    45.63\n",
      "| epoch  31 |   200/  558 batches | lr 0.00 | loss  3.96 | ppl    52.50\n",
      "| epoch  31 |   300/  558 batches | lr 0.00 | loss  3.89 | ppl    48.95\n",
      "| epoch  31 |   400/  558 batches | lr 0.00 | loss  3.95 | ppl    51.73\n",
      "| epoch  31 |   500/  558 batches | lr 0.00 | loss  3.73 | ppl    41.50\n",
      "Epoch: 32 | Epoch Time: 1m 22s\n",
      "| epoch  32 |   100/  558 batches | lr 0.00 | loss  3.75 | ppl    42.33\n",
      "| epoch  32 |   200/  558 batches | lr 0.00 | loss  3.87 | ppl    47.79\n",
      "| epoch  32 |   300/  558 batches | lr 0.00 | loss  3.85 | ppl    47.14\n",
      "| epoch  32 |   400/  558 batches | lr 0.00 | loss  3.95 | ppl    52.05\n",
      "| epoch  32 |   500/  558 batches | lr 0.00 | loss  3.74 | ppl    41.95\n",
      "Epoch: 33 | Epoch Time: 1m 22s\n",
      "| epoch  33 |   100/  558 batches | lr 0.00 | loss  3.77 | ppl    43.17\n",
      "| epoch  33 |   200/  558 batches | lr 0.00 | loss  3.90 | ppl    49.51\n",
      "| epoch  33 |   300/  558 batches | lr 0.00 | loss  3.82 | ppl    45.47\n",
      "| epoch  33 |   400/  558 batches | lr 0.00 | loss  3.90 | ppl    49.38\n",
      "| epoch  33 |   500/  558 batches | lr 0.00 | loss  3.66 | ppl    38.72\n",
      "Epoch: 34 | Epoch Time: 1m 22s\n",
      "| epoch  34 |   100/  558 batches | lr 0.00 | loss  3.76 | ppl    42.86\n",
      "| epoch  34 |   200/  558 batches | lr 0.00 | loss  3.91 | ppl    49.71\n",
      "| epoch  34 |   300/  558 batches | lr 0.00 | loss  3.85 | ppl    46.94\n",
      "| epoch  34 |   400/  558 batches | lr 0.00 | loss  3.87 | ppl    48.10\n",
      "| epoch  34 |   500/  558 batches | lr 0.00 | loss  3.66 | ppl    38.83\n",
      "Epoch: 35 | Epoch Time: 1m 22s\n",
      "| epoch  35 |   100/  558 batches | lr 0.00 | loss  3.72 | ppl    41.25\n",
      "| epoch  35 |   200/  558 batches | lr 0.00 | loss  3.84 | ppl    46.65\n",
      "| epoch  35 |   300/  558 batches | lr 0.00 | loss  3.74 | ppl    42.01\n",
      "| epoch  35 |   400/  558 batches | lr 0.00 | loss  3.83 | ppl    46.06\n",
      "| epoch  35 |   500/  558 batches | lr 0.00 | loss  3.61 | ppl    37.06\n",
      "Epoch: 36 | Epoch Time: 1m 22s\n",
      "| epoch  36 |   100/  558 batches | lr 0.00 | loss  3.73 | ppl    41.54\n",
      "| epoch  36 |   200/  558 batches | lr 0.00 | loss  3.81 | ppl    45.15\n",
      "| epoch  36 |   300/  558 batches | lr 0.00 | loss  3.79 | ppl    44.06\n",
      "| epoch  36 |   400/  558 batches | lr 0.00 | loss  3.85 | ppl    46.76\n",
      "| epoch  36 |   500/  558 batches | lr 0.00 | loss  3.59 | ppl    36.28\n",
      "Epoch: 37 | Epoch Time: 1m 22s\n",
      "| epoch  37 |   100/  558 batches | lr 0.00 | loss  3.61 | ppl    36.94\n",
      "| epoch  37 |   200/  558 batches | lr 0.00 | loss  3.80 | ppl    44.84\n",
      "| epoch  37 |   300/  558 batches | lr 0.00 | loss  3.79 | ppl    44.47\n",
      "| epoch  37 |   400/  558 batches | lr 0.00 | loss  3.85 | ppl    47.03\n",
      "| epoch  37 |   500/  558 batches | lr 0.00 | loss  3.65 | ppl    38.36\n",
      "Epoch: 38 | Epoch Time: 1m 22s\n",
      "| epoch  38 |   100/  558 batches | lr 0.00 | loss  3.68 | ppl    39.63\n",
      "| epoch  38 |   200/  558 batches | lr 0.00 | loss  3.86 | ppl    47.32\n",
      "| epoch  38 |   300/  558 batches | lr 0.00 | loss  3.74 | ppl    42.04\n",
      "| epoch  38 |   400/  558 batches | lr 0.00 | loss  3.84 | ppl    46.48\n",
      "| epoch  38 |   500/  558 batches | lr 0.00 | loss  3.59 | ppl    36.23\n",
      "Epoch: 39 | Epoch Time: 1m 22s\n",
      "| epoch  39 |   100/  558 batches | lr 0.00 | loss  3.68 | ppl    39.56\n",
      "| epoch  39 |   200/  558 batches | lr 0.00 | loss  3.79 | ppl    44.08\n",
      "| epoch  39 |   300/  558 batches | lr 0.00 | loss  3.73 | ppl    41.79\n",
      "| epoch  39 |   400/  558 batches | lr 0.00 | loss  3.78 | ppl    43.62\n",
      "| epoch  39 |   500/  558 batches | lr 0.00 | loss  3.61 | ppl    37.06\n",
      "Epoch: 40 | Epoch Time: 1m 22s\n",
      "| epoch  40 |   100/  558 batches | lr 0.00 | loss  3.64 | ppl    38.14\n",
      "| epoch  40 |   200/  558 batches | lr 0.00 | loss  3.80 | ppl    44.55\n",
      "| epoch  40 |   300/  558 batches | lr 0.00 | loss  3.72 | ppl    41.13\n",
      "| epoch  40 |   400/  558 batches | lr 0.00 | loss  3.80 | ppl    44.55\n",
      "| epoch  40 |   500/  558 batches | lr 0.00 | loss  3.57 | ppl    35.35\n",
      "Epoch: 41 | Epoch Time: 1m 22s\n",
      "| epoch  41 |   100/  558 batches | lr 0.00 | loss  3.63 | ppl    37.76\n",
      "| epoch  41 |   200/  558 batches | lr 0.00 | loss  3.80 | ppl    44.59\n",
      "| epoch  41 |   300/  558 batches | lr 0.00 | loss  3.72 | ppl    41.29\n",
      "| epoch  41 |   400/  558 batches | lr 0.00 | loss  3.82 | ppl    45.58\n",
      "| epoch  41 |   500/  558 batches | lr 0.00 | loss  3.54 | ppl    34.46\n",
      "Epoch: 42 | Epoch Time: 1m 22s\n",
      "| epoch  42 |   100/  558 batches | lr 0.00 | loss  3.64 | ppl    38.04\n",
      "| epoch  42 |   200/  558 batches | lr 0.00 | loss  3.82 | ppl    45.66\n",
      "| epoch  42 |   300/  558 batches | lr 0.00 | loss  3.75 | ppl    42.47\n",
      "| epoch  42 |   400/  558 batches | lr 0.00 | loss  3.76 | ppl    43.09\n",
      "| epoch  42 |   500/  558 batches | lr 0.00 | loss  3.60 | ppl    36.51\n",
      "Epoch: 43 | Epoch Time: 1m 22s\n",
      "| epoch  43 |   100/  558 batches | lr 0.00 | loss  3.65 | ppl    38.58\n",
      "| epoch  43 |   200/  558 batches | lr 0.00 | loss  3.83 | ppl    46.17\n",
      "| epoch  43 |   300/  558 batches | lr 0.00 | loss  3.76 | ppl    42.75\n",
      "| epoch  43 |   400/  558 batches | lr 0.00 | loss  3.79 | ppl    44.47\n",
      "| epoch  43 |   500/  558 batches | lr 0.00 | loss  3.52 | ppl    33.91\n",
      "Epoch: 44 | Epoch Time: 1m 22s\n",
      "| epoch  44 |   100/  558 batches | lr 0.00 | loss  3.63 | ppl    37.70\n",
      "| epoch  44 |   200/  558 batches | lr 0.00 | loss  3.73 | ppl    41.52\n",
      "| epoch  44 |   300/  558 batches | lr 0.00 | loss  3.73 | ppl    41.73\n",
      "| epoch  44 |   400/  558 batches | lr 0.00 | loss  3.73 | ppl    41.67\n",
      "| epoch  44 |   500/  558 batches | lr 0.00 | loss  3.56 | ppl    35.10\n",
      "Epoch: 45 | Epoch Time: 1m 22s\n",
      "| epoch  45 |   100/  558 batches | lr 0.00 | loss  3.62 | ppl    37.45\n",
      "| epoch  45 |   200/  558 batches | lr 0.00 | loss  3.78 | ppl    43.95\n",
      "| epoch  45 |   300/  558 batches | lr 0.00 | loss  3.66 | ppl    38.84\n",
      "| epoch  45 |   400/  558 batches | lr 0.00 | loss  3.70 | ppl    40.44\n",
      "| epoch  45 |   500/  558 batches | lr 0.00 | loss  3.48 | ppl    32.54\n",
      "Epoch: 46 | Epoch Time: 1m 22s\n",
      "| epoch  46 |   100/  558 batches | lr 0.00 | loss  3.66 | ppl    38.92\n",
      "| epoch  46 |   200/  558 batches | lr 0.00 | loss  3.74 | ppl    41.94\n",
      "| epoch  46 |   300/  558 batches | lr 0.00 | loss  3.68 | ppl    39.82\n",
      "| epoch  46 |   400/  558 batches | lr 0.00 | loss  3.72 | ppl    41.46\n",
      "| epoch  46 |   500/  558 batches | lr 0.00 | loss  3.48 | ppl    32.44\n",
      "Epoch: 47 | Epoch Time: 1m 22s\n",
      "| epoch  47 |   100/  558 batches | lr 0.00 | loss  3.65 | ppl    38.29\n",
      "| epoch  47 |   200/  558 batches | lr 0.00 | loss  3.77 | ppl    43.48\n",
      "| epoch  47 |   300/  558 batches | lr 0.00 | loss  3.67 | ppl    39.17\n",
      "| epoch  47 |   400/  558 batches | lr 0.00 | loss  3.76 | ppl    42.88\n",
      "| epoch  47 |   500/  558 batches | lr 0.00 | loss  3.51 | ppl    33.34\n",
      "Epoch: 48 | Epoch Time: 1m 22s\n",
      "| epoch  48 |   100/  558 batches | lr 0.00 | loss  3.61 | ppl    37.03\n",
      "| epoch  48 |   200/  558 batches | lr 0.00 | loss  3.68 | ppl    39.74\n",
      "| epoch  48 |   300/  558 batches | lr 0.00 | loss  3.68 | ppl    39.83\n",
      "| epoch  48 |   400/  558 batches | lr 0.00 | loss  3.73 | ppl    41.53\n",
      "| epoch  48 |   500/  558 batches | lr 0.00 | loss  3.55 | ppl    34.76\n",
      "Epoch: 49 | Epoch Time: 1m 22s\n",
      "| epoch  49 |   100/  558 batches | lr 0.00 | loss  3.58 | ppl    35.73\n",
      "| epoch  49 |   200/  558 batches | lr 0.00 | loss  3.64 | ppl    37.94\n",
      "| epoch  49 |   300/  558 batches | lr 0.00 | loss  3.65 | ppl    38.37\n",
      "| epoch  49 |   400/  558 batches | lr 0.00 | loss  3.71 | ppl    41.03\n",
      "| epoch  49 |   500/  558 batches | lr 0.00 | loss  3.50 | ppl    33.24\n",
      "Epoch: 50 | Epoch Time: 1m 22s\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "patience = 2\n",
    "log_interval = 100\n",
    "plot_every = 10\n",
    "all_loses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train(model, train_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQmPSEELeklc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPq1LtBU8AmZWFN3WqmWfPi",
   "name": "GRU_token.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}